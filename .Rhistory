#### *The number of the communities according to **Zachary's algorithm** is:*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(memb)
```
#### *The graph below shows the **distribution** of members in communities*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
membTable <- table(memb)
barplot(membTable, col=terrain.colors(7) , main="Bar plot for Number of Members\nVS\nCommunity",
xlab="#Community", ylab = "Number of Members")
memb
```
#### *Modularity*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
fc$modularity
```
#### *Which one had the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# get the max modularity for this graph:
which.max(fc$modularity)
```
#### *What the value of the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(fc$modularity)
```
___
### *ii. Second Algorithm - Girvan-Newman community detection algorithm*
#### *The graph shown below describes the distribution of vertices communities, by color depending, according to **Girvan-Newman algorithm***
```{r,echo=FALSE,warning=FALSE,message=FALSE}
fc <- edge.betweenness.community(greyAnatomy)
memb<-membership(fc)
plot(greyAnatomy, layout=lay, vertex.size=5, vertex.label=NA, vertex.color=memb+1, asp=FALSE)
```
#### *The number of the communities according to **Girvan-Newman** is:*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(memb)
```
#### *The graph below shows the **distribution** of members in communities*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
membTable <- table(memb)
barplot(membTable, col=terrain.colors(7) , main="Bar plot for Number of Members\nVS\nCommunity",
xlab="#Community", ylab = "Number of Members")
memb
```
#### *Modularity*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
fc$modularity
```
#### *Which one had the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# get the max modularity for this graph:
which.max(fc$modularity)
```
#### *What the value of the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(fc$modularity)
```
---
title: "Network Analysis - Greyâ€™s Anatomy Network of Romance"
output: html_document
---
```{r, echo=FALSE, cache=FALSE}
options(width = 200)
```
___
#### *Authors:* Ronit Yoari & Orel Swisa
#### *Date*: May 3, 2016
#### *The Dataset [Source](http://www.babelgraph.org/data/ga_edgelist.csv)*
___
## *Data Description*
### *Vertices*
```{r, echo=FALSE, message=FALSE}
library("downloader")
folder<-".\\data\\ga_edgelist.csv"
if(!file.exists(folder)){
# Download a csv file from the Internet:
fileURL <- "http://www.babelgraph.org/data/ga_edgelist.csv"
download.file(fileURL, destfile = folder)
# Always record the date
dateDownloaded <- date()
dateDownloaded
}
library(igraph)
# Read from local drive
ga.data <- read.csv(folder,header=T)
g <- graph.data.frame(ga.data,directed=FALSE)
V(g)$name
```
### *Edges*
```{r,echo=FALSE, message=FALSE}
E(g)
```
### *Summary*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
summary(g)
```
___
## *Centrality*
___
### *i. Betweeness*
```{r,echo=FALSE}
btw <- betweenness(g)
# rescale values to match the elements of a color vector
btw.score <- round( (btw - min(btw)) * length(btw) / max(btw) ) + 1
# create color vector, use rev to make red "hot"
btw.colors <- rev(heat.colors(max(btw.score)))
V(g)$color <- btw.colors[ btw.score ]
g$layout <- layout.fruchterman.reingold(g)
plot(g)
which.max(btw)
max(btw)
```
___
### *ii. Closeness*
```{r,echo=FALSE}
clo <- closeness(g)
# rescale values to match the elements of a color vector
clo.score <- round( (clo - min(clo)) * length(clo) / max(clo) ) + 1
# create color vector, use rev to make red "hot"
clo.colors <- rev(heat.colors(max(clo.score)))
V(g)$color <- clo.colors[ clo.score ]
plot(g)
which.max(clo)
max(clo)
```
___
### *iii. Eigencetor*
```{r,echo=FALSE}
temp <- eigen_centrality(g)
evc<-temp$vector
# rescale values to match the elements of a color vector
evc.score <- round( (evc - min(evc)) * length(evc) / max(evc) ) + 1
# create color vector, use rev to make red "hot"
evc.colors <- rev(heat.colors(max(evc.score)))
V(g)$color <- evc.colors[ evc.score ]
plot(g)
which.max(evc)
max(evc)
```
___
## *Identifying communities*
___
```{r,echo=FALSE,message=FALSE}
# Remove self-loops is exist
greyAnatomy <- simplify(g)
# Set seed (for layout) - so it won't be randomal
set.seed(200)
```
### *i. Zachary's algorithm*
#### *The graph shown below describes the distribution of vertices communities, by color depending, according to **Zachary's algorithm***
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# Community strucure via short random walks
fc <- walktrap.community(greyAnatomy)
memb<-membership(fc)
lay <- layout.fruchterman.reingold(greyAnatomy)
plot(greyAnatomy, layout=lay, vertex.size=5, vertex.label=NA, vertex.color=memb+1, asp=FALSE)
```
#### *The number of the communities according to **Zachary's algorithm** is:*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(memb)
```
#### *The graph below shows the **distribution** of members in communities*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
membTable <- table(memb)
barplot(membTable, col=terrain.colors(7) , main="Bar plot for Number of Members\nVS\nCommunity",
xlab="#Community", ylab = "Number of Members")
memb
```
#### *Modularity*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
fc$modularity
```
#### *Which one had the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# get the max modularity for this graph:
which.max(fc$modularity)
```
#### *What the value of the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(fc$modularity)
```
___
### *ii. Second Algorithm - Girvan-Newman community detection algorithm*
#### *The graph shown below describes the distribution of vertices communities, by color depending, according to **Girvan-Newman algorithm***
```{r,echo=FALSE,warning=FALSE,message=FALSE}
fc <- edge.betweenness.community(greyAnatomy)
memb<-membership(fc)
plot(greyAnatomy, layout=lay, vertex.size=5, vertex.label=NA, vertex.color=memb+1, asp=FALSE)
```
#### *The number of the communities according to **Girvan-Newman** is:*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(memb)
```
#### *The graph below shows the **distribution** of members in communities*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
membTable <- table(memb)
barplot(membTable, col=terrain.colors(7) , main="Bar plot for Number of Members\nVS\nCommunity",
xlab="#Community", ylab = "Number of Members")
memb
```
#### *Modularity*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
fc$modularity
```
#### *Which one had the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# get the max modularity for this graph:
which.max(fc$modularity)
```
#### *What the value of the max modularity?*
```{r,echo=FALSE,warning=FALSE,message=FALSE}
max(fc$modularity)
```
#### *Authors:* Ronit Yoari & Orel Swisa
options(width = 200)
```
#### *Date*: May 3, 2016
```{r, echo=FALSE, message=FALSE}
library("downloader")
library(twitteR)
library(tm)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-".\\data\\rdatammining.Rdata.gz"
if(!file.exists(folder)){
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
# rdmTweets <- userTimeline("cnn", n=1000)
# get 500 tweets with the hashtag #cnn
rdmTweets <- searchTwitter("@cnn", n = 1000 ,since = '2016-04-27 00:00', until = '2016-04-28 00:00')
save(rdmTweets, file=folder, compress=TRUE)
} else{
load(folder)
}
library("downloader")
library(twitteR)
library(tm)
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-".\\data\\rdatammining.Rdata.gz"
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
library("downloader")
library(twitteR)
library(tm)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-".\\data\\rdatammining.Rdata.gz"
if(!file.exists(folder)){
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
# rdmTweets <- userTimeline("cnn", n=1000)
# get 500 tweets with the hashtag #cnn
rdmTweets <- searchTwitter("@cnn", n = 1000 ,since = '2016-04-27 00:00', until = '2016-04-28 00:00')
save(rdmTweets, file=folder, compress=TRUE)
} else{
load(folder)
}
cnn = rdmTweets
head(cnn)
# Get the Date
dateAPI <- date()
dateAPI
data_mining_CNN <- twListToDF(cnn)
# Remove stop words :
data_mining_CNN2 = stringr::str_replace_all(data_mining_CNN$text,"[^[:graph:]]", " ")
# get the body of SuperClasico hashtag
data_mining_corpus_CNN = tm::Corpus(VectorSource(data_mining_CNN2))
cnn_dtm = TermDocumentMatrix(data_mining_corpus_CNN, control = list(minWordLength = 4, removePunctuation=TRUE, stopwords = c(stopwords('english')), removeNumbers=TRUE,tolower=TRUE,stripWhitespace=TRUE))
te<-inspect(cnn_dtm)
termsMet1 <- te %*% t(te)
View(termsMet1)
head(cnn)
setwd("~/Google Drive/DataScientist/Ass3/Q2 - Twitter CNN")
library(tm)
library(twitteR)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
folder<-"SuperClasico.Rdata.gz"
if(!file.exists(folder)){
# get 500 tweets with the hashtag #SuperClasico
SuperClasico <- searchTwitter("#SuperClasico", n = 500 ,since = '2016-04-02 00:00', until = '2016-04-03 00:00')
save(SuperClasico, file=folder, compress=TRUE)
} else{
load(folder)
}
head(SuperClasico)
# Get the Date
dateAPI <- date()
dateAPI
library("downloader")
library(twitteR)
library(tm)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-".\\data\\rdatammining.Rdata.gz"
if(!file.exists(folder)){
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
# rdmTweets <- userTimeline("cnn", n=1000)
# get 500 tweets with the hashtag #cnn
rdmTweets <- searchTwitter("@cnn", n = 1000 ,since = '2016-04-27 00:00', until = '2016-04-28 00:00')
save(rdmTweets, file=folder, compress=TRUE)
} else{
load(folder)
}
cnn = rdmTweets
head(cnn)
# Get the Date
dateAPI <- date()
dateAPI
data_mining_CNN <- twListToDF(cnn)
# Remove stop words :
data_mining_CNN2 = stringr::str_replace_all(data_mining_CNN$text,"[^[:graph:]]", " ")
# get the body of SuperClasico hashtag
data_mining_corpus_CNN = tm::Corpus(VectorSource(data_mining_CNN2))
cnn_dtm = TermDocumentMatrix(data_mining_corpus_CNN, control = list(minWordLength = 4, removePunctuation=TRUE, stopwords = c(stopwords('english')), removeNumbers=TRUE,tolower=TRUE,stripWhitespace=TRUE))
te<-inspect(cnn_dtm)
termsMet1 <- te %*% t(te)
View(termsMet1)
y
library("downloader")
library(twitteR)
library(tm)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-"rdatammining.Rdata.gz"
if(!file.exists(folder)){
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
# rdmTweets <- userTimeline("cnn", n=1000)
# get 500 tweets with the hashtag #cnn
rdmTweets <- searchTwitter("@cnn", n = 1000 ,since = '2016-04-27 00:00', until = '2016-04-28 00:00')
save(rdmTweets, file=folder, compress=TRUE)
} else{
load(folder)
}
cnn = rdmTweets
head(cnn)
# Get the Date
dateAPI <- date()
dateAPI
data_mining_CNN <- twListToDF(cnn)
# Remove stop words :
data_mining_CNN2 = stringr::str_replace_all(data_mining_CNN$text,"[^[:graph:]]", " ")
# get the body of SuperClasico hashtag
data_mining_corpus_CNN = tm::Corpus(VectorSource(data_mining_CNN2))
cnn_dtm = TermDocumentMatrix(data_mining_corpus_CNN, control = list(minWordLength = 4, removePunctuation=TRUE, stopwords = c(stopwords('english')), removeNumbers=TRUE,tolower=TRUE,stripWhitespace=TRUE))
te<-inspect(cnn_dtm)
termsMet1 <- te %*% t(te)
View(termsMet1)
library("downloader")
library(twitteR)
library(tm)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-"rdatammining.Rdata.gz"
if(!file.exists(folder)){
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
# rdmTweets <- userTimeline("cnn", n=1000)
# get 500 tweets with the hashtag #cnn
cnn <- searchTwitter("@cnn", n = 1000 ,since = '2016-04-27 00:00', until = '2016-04-28 00:00')
save(cnn, file=folder, compress=TRUE)
} else{
cnn<-load(folder)
}
head(cnn)
# Get the Date
dateAPI <- date()
dateAPI
data_mining_CNN <- twListToDF(cnn)
# Remove stop words :
data_mining_CNN2 = stringr::str_replace_all(data_mining_CNN$text,"[^[:graph:]]", " ")
# get the body of CNN hashtag
data_mining_corpus_CNN = tm::Corpus(VectorSource(data_mining_CNN2))
cnn_dtm = TermDocumentMatrix(data_mining_corpus_CNN, control = list(minWordLength = 4, removePunctuation=TRUE, stopwords = c(stopwords('english')), removeNumbers=TRUE,tolower=TRUE,stripWhitespace=TRUE))
te<-inspect(cnn_dtm)
termsMet1 <- te %*% t(te)
library("downloader")
library(twitteR)
library(tm)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-"rdatammining.Rdata.gz"
if(!file.exists(folder)){
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
# rdmTweets <- userTimeline("cnn", n=1000)
# get 500 tweets with the hashtag #cnn
cnn <- searchTwitter("@cnn", n = 1000 ,since = '2016-04-27 00:00', until = '2016-04-28 00:00')
save(cnn, file=folder, compress=TRUE)
} else{
load(folder)
}
head(cnn)
# Get the Date
dateAPI <- date()
dateAPI
data_mining_CNN <- twListToDF(cnn)
# Remove stop words :
data_mining_CNN2 = stringr::str_replace_all(data_mining_CNN$text,"[^[:graph:]]", " ")
# get the body of CNN hashtag
data_mining_corpus_CNN = tm::Corpus(VectorSource(data_mining_CNN2))
cnn_dtm = TermDocumentMatrix(data_mining_corpus_CNN, control = list(minWordLength = 4, removePunctuation=TRUE, stopwords = c(stopwords('english')), removeNumbers=TRUE,tolower=TRUE,stripWhitespace=TRUE))
te<-inspect(cnn_dtm)
termsMet1 <- te %*% t(te)
library("downloader")
library(twitteR)
library(tm)
# Twitter credentials
consumer_key <- "sjqXRKHfR2rD5VSPlYyscGTv1"
consumer_secret <- "fwB8pwlkc9K1YnrdKHt7pyZN0WzgsHiAoKUJJ9VpxCsoLKilWe"
access_token <- "1493511710-wmaNFnHNyn2FbutPkHCCwq61XOqYyuO7ODh4nsf"
access_secret <- "hpV8u6PCmH2eJUPbkUnKHPjgvTtPb5FnfUal64AM9phuR"
folder<-"rdatammining.Rdata.gz"
if(!file.exists(folder)){
# Connect Twitter account
sig <- twitteR::setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
# rdmTweets <- userTimeline("cnn", n=1000)
# get 500 tweets with the hashtag #cnn
cnn <- searchTwitter("@cnn", n = 1000 ,since = '2016-04-27 00:00', until = '2016-04-28 00:00')
save(cnn, file=folder, compress=TRUE)
} else{
load(folder)
}
head(cnn)
# Get the Date
dateAPI <- date()
dateAPI
data_mining_CNN <- twListToDF(cnn)
# Remove stop words :
data_mining_CNN2 = stringr::str_replace_all(data_mining_CNN$text,"[^[:graph:]]", " ")
# get the body of CNN hashtag
data_mining_corpus_CNN = tm::Corpus(VectorSource(data_mining_CNN2))
cnn_dtm = TermDocumentMatrix(data_mining_corpus_CNN, control = list(minWordLength = 4, removePunctuation=TRUE, stopwords = c(stopwords('english')), removeNumbers=TRUE,tolower=TRUE,stripWhitespace=TRUE))
te<-inspect(cnn_dtm)
termsMet1 <- te %*% t(te)
library(igraph)
terms<-c('trump','donald','hillary', 'clinton', 'ted', 'cruz','obama','bernie','fiorina','sanders','indiana','madrid','atletico', 'hastert','sarah', 'palin','kasich')
termsMet<-termsMet1[terms,terms]
termsMet
g <- graph.adjacency(termsMet, weighted=TRUE, mode = 'undirecte')
g <- set.edge.attribute(g, "weight", value=runif(ecount(g)))
g <- simplify(g)
V(g)$color <- "seagreen1"
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
# set seed to make the layout reproducible
set.seed(2000)
layout1 <- layout.fruchterman.reingold(g)
# V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
# plot the graph in layout1
plot(g, layout=layout1,vertex.size=30)
# V(g)$label <- NA
# remove labels for now
V(g)$size <- degree(g) * 2
# multiply by 2 for scale
plot(g)
library(igraph)
terms<-c('trump','donald','hillary', 'clinton', 'ted', 'cruz','obama','bernie','fiorina','sanders','indiana','madrid','atletico', 'hastert','sarah', 'palin','kasich')
termsMet<-termsMet1[terms,terms]
termsMet
g <- graph.adjacency(termsMet, weighted=TRUE, mode = 'undirecte')
plot(g)
library(igraph)
terms<-c('trump','donald','hillary', 'clinton', 'ted', 'cruz','obama','bernie','fiorina','sanders','indiana','madrid','atletico', 'hastert','sarah', 'palin','kasich')
termsMet<-termsMet1[terms,terms]
termsMet
g <- graph.adjacency(termsMet, weighted=TRUE, mode = 'undirecte')
g <- set.edge.attribute(g, "weight", value=runif(ecount(g)))
g <- simplify(g)
plot(g)
